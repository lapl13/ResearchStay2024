# **Week 4: Maximum Likelihood Estimation (MLE)**


## **Objectives:**

Week 4 focuses on Maximum Likelihood Estimation, concentrating on the conceptual, technical, and mathematical aspects through the readings
To gain some hands-on experience, we implemented through five progressively challenging examples, briefy explained below:
 
### Example 1 - MLE for Normal Distribution: 
We estimate the mean and standard deviation of a normal distribution based on a set of data points. The mean is calculated as the average of the data, and the standard deviation measures the spread of the data.

### Example 2 - MLE for Exponential Distribution: 
Here, we estimate the rate parameter (lambda) of an exponential distribution. The rate represents the number of occurrences per unit time and is calculated as the reciprocal of the mean of the data.

### Example 3 - MLE for Poisson Distribution: 
In this case, we estimate the rate parameter (lambda) of a Poisson distribution from count data. The rate parameter is simply the average count of events observed.

### Example 4 - MLE for Gamma Distribution: 
We estimate the shape and scale parameters of a gamma distribution. The shape parameter determines the shape of the distribution, while the scale parameter controls the spread. These are calculated based on the mean and variance of the data.

### Example 5 - MLE for Mixture of Normals: In this example, we estimate the parameters of a mixture of two normal distributions. Each component has its mean and standard deviation. We assume equal weights for the two components and calculate their parameters using the mean and standard deviation of the input data.


## **Professor Provided Readings:**

[Tutorial on maximum likelihood estimation](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=4ab2cfe6766a5007b2fcf8cfffbf7fb566c077f4)

[Bias reduction of maximum likelihood estimates](https://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf)

[Maximum likelihood estimation of logistic regression models: theory and implementation](https://saedsayad.com/docs/mlelr.pdf)


## **Self-Researched Readings:**  

[Recent Advances in Stochastic Gradient Descent in Deep Learning]([https://github.com/rycolab/ngram_regularizers](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://www.mdpi.com/2227-7390/11/3/682&ved=2ahUKEwja8qqYk82IAxUv4ckDHS02C0wQFnoECBUQAQ&usg=AOvVaw0hSJwvpWDvVG7Vv1PebMLb))
