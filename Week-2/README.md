# **Week 2: Text Preprocessing and Tokenization**


## **Objectives:**

Week 2 of the course focuses on Text Preprocessing and Tokenization, fundamental techniques used in Natural Language Processing (NLP).
We learned how to prepare textual data for analysis by performing essential preprocessing steps, such as lowercasing and punctuation removal. Additionally, we explored the concept of tokenization, which involves breaking text into individual units (tokens) for further analysis. We also performed a practical exercise in implementing a simple tokenizer to gain hands-on experience.


## **Professor Provided Readings:**

[What is a word, What is a sentence? Problems of Tokenization] (https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=aa80793d1d41d5241017a8fc755b7efc362a6439)

[Text preprocessing for text mining in organizational research](https://journals.sagepub.com/doi/pdf/10.1177/1094428120971683)

[Text preprocessing for unsupervised learning](https://arthurspirling.org/documents/preprocessing.pdf)


## **Self-Researched Readings:**  

[Analysis of Deep Learning Model Combinations and Tokenization Approaches in Sentiment Classification](https://www.scopus.com/record/display.uri?eid=2-s2.0-85089247395&origin=resultslist)

[Is text preprocessing still worth the time? A comparative survey on the influence of popular preprocessing methods on Transformers and traditional classifiers](https://doi.org/10.1016/j.knosys.2023.109374)

[Tokenizer Choice For LLM Training: Negligible or Crucial?](https://www.aclweb.org/anthology/2024.acl-main.524/)
